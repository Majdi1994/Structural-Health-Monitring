{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "#from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as ptl\n",
    "from utils import MyDataset\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoolModel(ptl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(CoolModel, self).__init__()\n",
    "        self.data_dir='data/train_test_dataset_1024.pt'\n",
    "        self.my_data = torch.load(self.data_dir)\n",
    "        # not the best model...\n",
    "        self.hidden_dim = 30\n",
    "        self.input_dim = 1\n",
    "        self.dropout=0.5\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.input_dim, 8, kernel_size=7, stride=1, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, self.hidden_dim))\n",
    "        self.Classifier= nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim//2) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(self.hidden_dim//2, 11))\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.unsqueeze(1)\n",
    "        features = self.encoder(src)\n",
    "        predictions = self.Classifier(features)\n",
    "        return predictions, features\n",
    "\n",
    "    def my_loss(self, y_hat, y):\n",
    "        return F.cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat,_ = self.forward(x)\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        y_hat,_ = self.forward(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat,_ = self.forward(x)\n",
    "        return {'val_loss': self.my_loss(y_hat, y)}\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat,_ = self.forward(x)\n",
    "        return {'test_loss': F.cross_entropy(y_hat, y)} \n",
    "    def test_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [torch.optim.Adam(self.parameters(), lr=0.02)]\n",
    "\n",
    "    @ptl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        data= self.my_data['train_data'],  self.my_data['train_labels']\n",
    "        self.dataset = MyDataset(self.my_data['train_data'],  self.my_data['train_labels'])\n",
    "        return DataLoader(self.dataset, batch_size=10, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    @ptl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        test_dl = DataLoader(MyDataset(self.my_data['test_data'],  self.my_data['test_labels']), batch_size=10, shuffle=False, drop_last=False)\n",
    "        return test_dl \n",
    "\n",
    "    @ptl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test_dl = DataLoader(MyDataset(self.my_data['test_data'],  self.my_data['test_labels']), batch_size=10, shuffle=False, drop_last=False)\n",
    "        return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "            Name         Type Params\n",
      "0        encoder   Sequential    8 K\n",
      "1      encoder.0       Conv1d   64  \n",
      "2      encoder.1  BatchNorm1d   16  \n",
      "3      encoder.2         ReLU    0  \n",
      "4      encoder.3       Conv1d  200  \n",
      "5      encoder.4  BatchNorm1d   16  \n",
      "6      encoder.5         ReLU    0  \n",
      "7      encoder.6       Conv1d  200  \n",
      "8      encoder.7  BatchNorm1d   16  \n",
      "9      encoder.8         ReLU    0  \n",
      "10     encoder.9       Conv1d  200  \n",
      "11    encoder.10  BatchNorm1d   16  \n",
      "12    encoder.11         ReLU    0  \n",
      "13    encoder.12       Conv1d  200  \n",
      "14    encoder.13  BatchNorm1d   16  \n",
      "15    encoder.14         ReLU    0  \n",
      "16    encoder.15       Conv1d  200  \n",
      "17    encoder.16      Flatten    0  \n",
      "18    encoder.17       Linear    7 K\n",
      "19    Classifier   Sequential    1 K\n",
      "20  Classifier.0       Linear  930  \n",
      "21  Classifier.1         ReLU    0  \n",
      "22  Classifier.2      Dropout    0  \n",
      "23  Classifier.3       Linear  465  \n",
      "24  Classifier.4         ReLU    0  \n",
      "25  Classifier.5      Dropout    0  \n",
      "26  Classifier.6       Linear  176  \n",
      "27            l1       Linear    7 K\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▍                                  | 3/275 [00:00<00:11, 23.87batch/s, batch_idx=2, loss=2.460, v_num=32]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|████████████████████████▊      | 220/275 [00:11<00:02, 20.82batch/s, batch_idx=219, loss=2.402, v_num=32]\n",
      "Epoch 1:  82%|█████████████████████████▍     | 226/275 [00:11<00:01, 26.30batch/s, batch_idx=219, loss=2.402, v_num=32]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████    | 240/275 [00:11<00:01, 34.71batch/s, batch_idx=219, loss=2.402, v_num=32]\u001b[A\n",
      "Epoch 1:  93%|████████████████████████████▋  | 255/275 [00:11<00:00, 45.08batch/s, batch_idx=219, loss=2.402, v_num=32]\u001b[A\n",
      "Epoch 1: 100%|███████████████████████████████| 275/275 [00:11<00:00, 57.49batch/s, batch_idx=219, loss=2.402, v_num=32]\u001b[A\n",
      "Epoch 1: 100%|███████████████████████████████| 275/275 [00:11<00:00, 23.72batch/s, batch_idx=219, loss=2.402, v_num=32]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed Ragab\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\pytorch_lightning\\callbacks\\pt_callbacks.py:314: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  ' skipping.', RuntimeWarning)\n",
      "C:\\Users\\Mohamed Ragab\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\pytorch_lightning\\callbacks\\pt_callbacks.py:144: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,train_loss,avg_val_loss\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    trainer = ptl.Trainer(max_epochs=1000)\t\n",
    "    model = CoolModel()\n",
    "    # ------------------------\n",
    "    # 3 START TRAINING\n",
    "    # ------------------------\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "            Name         Type Params\n",
      "0        encoder   Sequential    8 K\n",
      "1      encoder.0       Conv1d   64  \n",
      "2      encoder.1  BatchNorm1d   16  \n",
      "3      encoder.2         ReLU    0  \n",
      "4      encoder.3       Conv1d  200  \n",
      "5      encoder.4  BatchNorm1d   16  \n",
      "6      encoder.5         ReLU    0  \n",
      "7      encoder.6       Conv1d  200  \n",
      "8      encoder.7  BatchNorm1d   16  \n",
      "9      encoder.8         ReLU    0  \n",
      "10     encoder.9       Conv1d  200  \n",
      "11    encoder.10  BatchNorm1d   16  \n",
      "12    encoder.11         ReLU    0  \n",
      "13    encoder.12       Conv1d  200  \n",
      "14    encoder.13  BatchNorm1d   16  \n",
      "15    encoder.14         ReLU    0  \n",
      "16    encoder.15       Conv1d  200  \n",
      "17    encoder.16      Flatten    0  \n",
      "18    encoder.17       Linear    7 K\n",
      "19    Classifier   Sequential    1 K\n",
      "20  Classifier.0       Linear  930  \n",
      "21  Classifier.1         ReLU    0  \n",
      "22  Classifier.2      Dropout    0  \n",
      "23  Classifier.3       Linear  465  \n",
      "24  Classifier.4         ReLU    0  \n",
      "25  Classifier.5      Dropout    0  \n",
      "26  Classifier.6       Linear  176  \n",
      "27            l1       Linear    7 K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████| 55/55 [00:00<00:00, 308.09batch/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoolModel(ptl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(CoolModel, self).__init__()\n",
    "        self.data_dir='data/train_test_dataset_1024.pt'\n",
    "        self.my_data = torch.load(self.data_dir)\n",
    "        # not the best model...\n",
    "        self.hidden_dim = 30\n",
    "        self.input_dim = 1\n",
    "        self.dropout=0.5\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.input_dim, 8, kernel_size=7, stride=1, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 8, kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, self.hidden_dim))\n",
    "        self.Classifier= nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim//2) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(self.hidden_dim//2, 11))\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.unsqueeze(1)\n",
    "        features = self.encoder(src)\n",
    "        predictions = self.Classifier(features)\n",
    "        return predictions, features\n",
    "\n",
    "    def my_loss(self, y_hat, y):\n",
    "        return F.cross_entropy(y_hat, y)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, target = batch\n",
    "        output,_ = self.forward(images)\n",
    "        loss_val = F.mse_loss(output, target)\n",
    "        acc1, acc5 = self.__accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            acc1 = acc1.unsqueeze(0)\n",
    "            acc5 = acc5.unsqueeze(0)\n",
    "\n",
    "        tqdm_dict = {'train_loss': loss_val}\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val,\n",
    "            'acc1': acc1,\n",
    "            'acc5': acc5,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return output\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, target = batch\n",
    "        output,_ = self.forward(images)\n",
    "        loss_val = F.mse_loss(output, target)\n",
    "        acc1, acc5 = self.__accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            acc1 = acc1.unsqueeze(0)\n",
    "            acc5 = acc5.unsqueeze(0)\n",
    "        tqdm_dict = {'val_loss': loss_val}\n",
    "        output = OrderedDict({\n",
    "            'val_loss': loss_val,\n",
    "            'val_acc1': acc1,\n",
    "            'val_acc5': acc5,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "\n",
    "        return output\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "\n",
    "        tqdm_dict = {}\n",
    "\n",
    "        for metric_name in [\"val_loss\", \"val_acc1\", \"val_acc5\"]:\n",
    "            metric_total = 0\n",
    "\n",
    "            for output in outputs:\n",
    "                metric_value = output[metric_name]\n",
    "\n",
    "                # reduce manually when using dp\n",
    "                if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                    metric_value = torch.mean(metric_value)\n",
    "\n",
    "                metric_total += metric_value\n",
    "\n",
    "            tqdm_dict[metric_name] = metric_total / len(outputs)\n",
    "\n",
    "        result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': tqdm_dict[\"val_loss\"]}\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def __accuracy(cls, output, target, topk=(1,)):\n",
    "        \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "        with torch.no_grad():\n",
    "            maxk = max(topk)\n",
    "            batch_size = target.size(0)\n",
    "\n",
    "            _, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            res = []\n",
    "            for k in topk:\n",
    "                correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            return res  \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        y_hat,_ = self.forward(x)\n",
    "        return {'test_loss': F.cross_entropy(y_hat, y)} \n",
    "    def test_end(self, outputs):\n",
    "        # OPTIONAL\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [torch.optim.Adam(self.parameters(), lr=0.02)]\n",
    "\n",
    "    @ptl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        data= self.my_data['train_data'],  self.my_data['train_labels']\n",
    "        self.dataset = MyDataset(self.my_data['train_data'],  self.my_data['train_labels'])\n",
    "        return DataLoader(self.dataset, batch_size=10, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    @ptl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        test_dl = DataLoader(MyDataset(self.my_data['test_data'],  self.my_data['test_labels']), batch_size=10, shuffle=False, drop_last=False)\n",
    "        return test_dl \n",
    "\n",
    "    @ptl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        test_dl = DataLoader(MyDataset(self.my_data['test_data'],  self.my_data['test_labels']), batch_size=10, shuffle=False, drop_last=False)\n",
    "        return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "            Name         Type Params\n",
      "0        encoder   Sequential    8 K\n",
      "1      encoder.0       Conv1d   64  \n",
      "2      encoder.1  BatchNorm1d   16  \n",
      "3      encoder.2         ReLU    0  \n",
      "4      encoder.3       Conv1d  200  \n",
      "5      encoder.4  BatchNorm1d   16  \n",
      "6      encoder.5         ReLU    0  \n",
      "7      encoder.6       Conv1d  200  \n",
      "8      encoder.7  BatchNorm1d   16  \n",
      "9      encoder.8         ReLU    0  \n",
      "10     encoder.9       Conv1d  200  \n",
      "11    encoder.10  BatchNorm1d   16  \n",
      "12    encoder.11         ReLU    0  \n",
      "13    encoder.12       Conv1d  200  \n",
      "14    encoder.13  BatchNorm1d   16  \n",
      "15    encoder.14         ReLU    0  \n",
      "16    encoder.15       Conv1d  200  \n",
      "17    encoder.16      Flatten    0  \n",
      "18    encoder.17       Linear    7 K\n",
      "19    Classifier   Sequential    1 K\n",
      "20  Classifier.0       Linear  930  \n",
      "21  Classifier.1         ReLU    0  \n",
      "22  Classifier.2      Dropout    0  \n",
      "23  Classifier.3       Linear  465  \n",
      "24  Classifier.4         ReLU    0  \n",
      "25  Classifier.5      Dropout    0  \n",
      "26  Classifier.6       Linear  176  \n",
      "27            l1       Linear    7 K\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▏                  | 3/275 [00:00<00:11, 24.46batch/s, batch_idx=2, loss=2.465, train_loss=2.5, v_num=44]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|███████████▏  | 220/275 [00:11<00:02, 18.70batch/s, batch_idx=219, loss=2.407, train_loss=2.41, v_num=44]\n",
      "Epoch 1:  84%|███████████▊  | 232/275 [00:12<00:01, 24.90batch/s, batch_idx=219, loss=2.407, train_loss=2.41, v_num=44]\u001b[A\n",
      "Epoch 1:  89%|████████████▍ | 244/275 [00:12<00:00, 32.65batch/s, batch_idx=219, loss=2.407, train_loss=2.41, v_num=44]\u001b[A\n",
      "Epoch 1:  93%|█████████████ | 257/275 [00:12<00:00, 41.92batch/s, batch_idx=219, loss=2.407, train_loss=2.41, v_num=44]\u001b[A\n",
      "Epoch 1:  98%|█████████████▋| 270/275 [00:12<00:00, 52.38batch/s, batch_idx=219, loss=2.407, train_loss=2.41, v_num=44]\u001b[A\n",
      "Epoch 1: 100%|█| 275/275 [00:12<00:00, 52.38batch/s, batch_idx=219, loss=2.407, train_loss=2.41, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 2:  80%|▊| 220/275 [00:11<00:02, 19.53batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 2:  84%|▊| 232/275 [00:11<00:01, 26.00batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 2:  89%|▉| 246/275 [00:11<00:00, 34.36batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 2:  94%|▉| 259/275 [00:11<00:00, 44.06batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 2:  99%|▉| 272/275 [00:12<00:00, 54.43batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 2: 100%|█| 275/275 [00:12<00:00, 54.43batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 3:  80%|▊| 220/275 [00:11<00:02, 18.94batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 3:  83%|▊| 227/275 [00:11<00:01, 24.43batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 3:  87%|▊| 239/275 [00:11<00:01, 31.93batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 3:  92%|▉| 252/275 [00:11<00:00, 41.01batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 3:  96%|▉| 265/275 [00:11<00:00, 51.38batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 3: 100%|█| 275/275 [00:11<00:00, 51.38batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=10.\u001b[A\n",
      "Epoch 4:  80%|▊| 220/275 [00:12<00:02, 18.77batch/s, batch_idx=219, loss=2.407, train_loss=2.42, v_num=44, val_acc1=10.\u001b[A\n",
      "Epoch 4:  83%|▊| 229/275 [00:12<00:01, 24.44batch/s, batch_idx=219, loss=2.407, train_loss=2.42, v_num=44, val_acc1=10.\u001b[A\n",
      "Epoch 4:  88%|▉| 243/275 [00:12<00:00, 32.47batch/s, batch_idx=219, loss=2.407, train_loss=2.42, v_num=44, val_acc1=10.\u001b[A\n",
      "Epoch 4:  92%|▉| 252/275 [00:12<00:00, 39.87batch/s, batch_idx=219, loss=2.407, train_loss=2.42, v_num=44, val_acc1=10.\u001b[A\n",
      "Epoch 4:  96%|▉| 265/275 [00:12<00:00, 50.24batch/s, batch_idx=219, loss=2.407, train_loss=2.42, v_num=44, val_acc1=10.\u001b[A\n",
      "Epoch 4: 100%|█| 275/275 [00:12<00:00, 50.24batch/s, batch_idx=219, loss=2.407, train_loss=2.42, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 5:  80%|▊| 220/275 [00:12<00:02, 18.79batch/s, batch_idx=219, loss=2.400, train_loss=2.42, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 5:  82%|▊| 225/275 [00:12<00:02, 23.47batch/s, batch_idx=219, loss=2.400, train_loss=2.42, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 5:  86%|▊| 236/275 [00:12<00:01, 30.71batch/s, batch_idx=219, loss=2.400, train_loss=2.42, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 5:  90%|▉| 248/275 [00:12<00:00, 39.42batch/s, batch_idx=219, loss=2.400, train_loss=2.42, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 5:  95%|▉| 262/275 [00:12<00:00, 49.89batch/s, batch_idx=219, loss=2.400, train_loss=2.42, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 5: 100%|█| 275/275 [00:12<00:00, 49.89batch/s, batch_idx=219, loss=2.400, train_loss=2.42, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 6:  80%|▊| 220/275 [00:12<00:02, 18.74batch/s, batch_idx=219, loss=2.404, train_loss=2.43, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 6:  84%|▊| 231/275 [00:12<00:01, 24.91batch/s, batch_idx=219, loss=2.404, train_loss=2.43, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 6:  87%|▊| 239/275 [00:12<00:01, 31.22batch/s, batch_idx=219, loss=2.404, train_loss=2.43, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 6:  92%|▉| 254/275 [00:12<00:00, 40.79batch/s, batch_idx=219, loss=2.404, train_loss=2.43, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 6:  97%|▉| 267/275 [00:12<00:00, 51.02batch/s, batch_idx=219, loss=2.404, train_loss=2.43, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 6: 100%|█| 275/275 [00:12<00:00, 51.02batch/s, batch_idx=219, loss=2.404, train_loss=2.43, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 7:  80%|▊| 220/275 [00:12<00:02, 18.37batch/s, batch_idx=219, loss=2.402, train_loss=2.4, v_num=44, val_acc1=7.82\u001b[A\n",
      "Epoch 7:  84%|▊| 230/275 [00:12<00:01, 24.12batch/s, batch_idx=219, loss=2.402, train_loss=2.4, v_num=44, val_acc1=7.82\u001b[A\n",
      "Epoch 7:  87%|▊| 240/275 [00:12<00:01, 30.98batch/s, batch_idx=219, loss=2.402, train_loss=2.4, v_num=44, val_acc1=7.82\u001b[A\n",
      "Epoch 7:  91%|▉| 249/275 [00:12<00:00, 38.38batch/s, batch_idx=219, loss=2.402, train_loss=2.4, v_num=44, val_acc1=7.82\u001b[A\n",
      "Epoch 7:  96%|▉| 264/275 [00:12<00:00, 49.28batch/s, batch_idx=219, loss=2.402, train_loss=2.4, v_num=44, val_acc1=7.82\u001b[A\n",
      "Epoch 7: 100%|█| 275/275 [00:12<00:00, 49.28batch/s, batch_idx=219, loss=2.402, train_loss=2.4, v_num=44, val_acc1=7.64\u001b[A\n",
      "Epoch 8:  80%|▊| 220/275 [00:12<00:02, 18.84batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=7.6\u001b[A\n",
      "Epoch 8:  84%|▊| 231/275 [00:12<00:01, 25.03batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=7.6\u001b[A\n",
      "Epoch 8:  88%|▉| 243/275 [00:12<00:00, 32.80batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=7.6\u001b[A\n",
      "Epoch 8:  95%|▉| 260/275 [00:12<00:00, 43.09batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=7.6\u001b[A\n",
      "Epoch 8: 100%|▉| 274/275 [00:12<00:00, 54.21batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=7.6\u001b[A\n",
      "Epoch 8: 100%|█| 275/275 [00:12<00:00, 54.21batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 9:  80%|▊| 220/275 [00:11<00:02, 18.76batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 9:  80%|▊| 221/275 [00:11<00:02, 20.33batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 9:  85%|▊| 234/275 [00:12<00:01, 27.13batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 9:  90%|▉| 247/275 [00:12<00:00, 35.49batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 9:  94%|▉| 258/275 [00:12<00:00, 44.29batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 9: 100%|█| 275/275 [00:12<00:00, 55.16batch/s, batch_idx=219, loss=2.403, train_loss=2.41, v_num=44, val_acc1=6.9\u001b[A\n",
      "Epoch 10:  80%|▊| 220/275 [00:11<00:02, 18.77batch/s, batch_idx=219, loss=2.403, train_loss=2.39, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 10:  84%|▊| 230/275 [00:11<00:01, 24.67batch/s, batch_idx=219, loss=2.403, train_loss=2.39, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 10:  87%|▊| 240/275 [00:11<00:01, 31.82batch/s, batch_idx=219, loss=2.403, train_loss=2.39, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 10:  91%|▉| 251/275 [00:12<00:00, 40.15batch/s, batch_idx=219, loss=2.403, train_loss=2.39, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 10:  95%|▉| 261/275 [00:12<00:00, 48.25batch/s, batch_idx=219, loss=2.403, train_loss=2.39, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 10:  99%|▉| 271/275 [00:12<00:00, 56.94batch/s, batch_idx=219, loss=2.403, train_loss=2.39, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 10: 100%|█| 275/275 [00:12<00:00, 56.94batch/s, batch_idx=219, loss=2.403, train_loss=2.39, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 11:  80%|▊| 220/275 [00:12<00:02, 18.43batch/s, batch_idx=219, loss=2.405, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 11:  82%|▊| 225/275 [00:12<00:02, 23.06batch/s, batch_idx=219, loss=2.405, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 11:  85%|▊| 235/275 [00:12<00:01, 29.43batch/s, batch_idx=219, loss=2.405, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 11:  89%|▉| 245/275 [00:12<00:00, 37.22batch/s, batch_idx=219, loss=2.405, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 11:  95%|▉| 260/275 [00:12<00:00, 47.84batch/s, batch_idx=219, loss=2.405, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|█| 275/275 [00:12<00:00, 58.05batch/s, batch_idx=219, loss=2.405, train_loss=2.4, v_num=44, val_acc1=9.0\u001b[A\n",
      "Epoch 12:  80%|▊| 220/275 [00:11<00:02, 18.88batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 12:  80%|▊| 221/275 [00:11<00:02, 20.47batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 12:  85%|▊| 234/275 [00:11<00:01, 27.35batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 12:  90%|▉| 247/275 [00:11<00:00, 35.61batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 12:  95%|▉| 262/275 [00:12<00:00, 45.93batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 12: 100%|█| 275/275 [00:12<00:00, 45.93batch/s, batch_idx=219, loss=2.405, train_loss=2.44, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 13:  80%|▊| 220/275 [00:11<00:02, 20.30batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 13:  81%|▊| 224/275 [00:11<00:02, 24.46batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 13:  86%|▊| 236/275 [00:11<00:01, 31.88batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 13:  90%|▉| 248/275 [00:11<00:00, 40.53batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 13:  96%|▉| 264/275 [00:11<00:00, 51.99batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 13: 100%|█| 275/275 [00:11<00:00, 51.99batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 14:  80%|▊| 220/275 [00:11<00:03, 17.94batch/s, batch_idx=219, loss=2.404, train_loss=2.4, v_num=44, val_acc1=9.0\u001b[A\n",
      "Epoch 14:  82%|▊| 226/275 [00:11<00:02, 22.91batch/s, batch_idx=219, loss=2.404, train_loss=2.4, v_num=44, val_acc1=9.0\u001b[A\n",
      "Epoch 14:  87%|▊| 238/275 [00:11<00:01, 30.13batch/s, batch_idx=219, loss=2.404, train_loss=2.4, v_num=44, val_acc1=9.0\u001b[A\n",
      "Epoch 14:  91%|▉| 250/275 [00:11<00:00, 38.76batch/s, batch_idx=219, loss=2.404, train_loss=2.4, v_num=44, val_acc1=9.0\u001b[A\n",
      "Epoch 14:  97%|▉| 266/275 [00:12<00:00, 49.98batch/s, batch_idx=219, loss=2.404, train_loss=2.4, v_num=44, val_acc1=9.0\u001b[A\n",
      "Epoch 14: 100%|█| 275/275 [00:12<00:00, 49.98batch/s, batch_idx=219, loss=2.404, train_loss=2.4, v_num=44, val_acc1=7.6\u001b[A\n",
      "Epoch 15:  80%|▊| 220/275 [00:11<00:02, 20.18batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 15:  80%|▊| 221/275 [00:11<00:02, 20.69batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 15:  85%|▊| 233/275 [00:11<00:01, 27.37batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 15:  90%|▉| 247/275 [00:11<00:00, 35.92batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 15: 100%|█| 275/275 [00:11<00:00, 46.11batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 16:  80%|▊| 220/275 [00:11<00:02, 19.16batch/s, batch_idx=219, loss=2.403, train_loss=2.4, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 16:  84%|▊| 232/275 [00:11<00:01, 25.54batch/s, batch_idx=219, loss=2.403, train_loss=2.4, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 16:  90%|▉| 248/275 [00:11<00:00, 34.13batch/s, batch_idx=219, loss=2.403, train_loss=2.4, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 16:  96%|▉| 264/275 [00:11<00:00, 44.57batch/s, batch_idx=219, loss=2.403, train_loss=2.4, v_num=44, val_acc1=7.8\u001b[A\n",
      "Epoch 16: 100%|█| 275/275 [00:11<00:00, 44.57batch/s, batch_idx=219, loss=2.403, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 17:  80%|▊| 220/275 [00:11<00:02, 19.88batch/s, batch_idx=219, loss=2.406, train_loss=2.41, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 17:  80%|▊| 221/275 [00:11<00:02, 21.65batch/s, batch_idx=219, loss=2.406, train_loss=2.41, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 17:  85%|▊| 233/275 [00:11<00:01, 28.68batch/s, batch_idx=219, loss=2.406, train_loss=2.41, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 17:  90%|▉| 248/275 [00:11<00:00, 37.84batch/s, batch_idx=219, loss=2.406, train_loss=2.41, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 17: 100%|█| 275/275 [00:11<00:00, 49.32batch/s, batch_idx=219, loss=2.406, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 18:  80%|▊| 220/275 [00:11<00:02, 19.94batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 18:  84%|▊| 232/275 [00:11<00:01, 26.55batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 18:  90%|▉| 247/275 [00:11<00:00, 35.21batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 18:  95%|▉| 261/275 [00:11<00:00, 45.33batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 18: 100%|█| 275/275 [00:11<00:00, 56.00batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 19:  80%|▊| 220/275 [00:11<00:02, 20.31batch/s, batch_idx=219, loss=2.406, train_loss=2.47, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 19:  82%|▊| 226/275 [00:11<00:01, 25.67batch/s, batch_idx=219, loss=2.406, train_loss=2.47, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 19:  87%|▊| 239/275 [00:11<00:01, 33.71batch/s, batch_idx=219, loss=2.406, train_loss=2.47, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 19:  93%|▉| 255/275 [00:11<00:00, 44.11batch/s, batch_idx=219, loss=2.406, train_loss=2.47, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 19: 100%|█| 275/275 [00:11<00:00, 56.61batch/s, batch_idx=219, loss=2.406, train_loss=2.47, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 20:  80%|▊| 220/275 [00:11<00:02, 19.84batch/s, batch_idx=219, loss=2.406, train_loss=2.35, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 20:  84%|▊| 232/275 [00:11<00:01, 26.40batch/s, batch_idx=219, loss=2.406, train_loss=2.35, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 20:  89%|▉| 246/275 [00:11<00:00, 34.72batch/s, batch_idx=219, loss=2.406, train_loss=2.35, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 20:  94%|▉| 258/275 [00:11<00:00, 44.04batch/s, batch_idx=219, loss=2.406, train_loss=2.35, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 20:  99%|▉| 273/275 [00:11<00:00, 55.84batch/s, batch_idx=219, loss=2.406, train_loss=2.35, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 20: 100%|█| 275/275 [00:11<00:00, 55.84batch/s, batch_idx=219, loss=2.406, train_loss=2.35, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 21:  80%|▊| 220/275 [00:11<00:02, 19.38batch/s, batch_idx=219, loss=2.407, train_loss=2.37, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 21:  82%|▊| 226/275 [00:11<00:01, 24.60batch/s, batch_idx=219, loss=2.407, train_loss=2.37, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 21:  87%|▊| 238/275 [00:11<00:01, 32.24batch/s, batch_idx=219, loss=2.407, train_loss=2.37, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 21:  92%|▉| 252/275 [00:11<00:00, 41.89batch/s, batch_idx=219, loss=2.407, train_loss=2.37, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 21:  96%|▉| 265/275 [00:11<00:00, 52.47batch/s, batch_idx=219, loss=2.407, train_loss=2.37, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 21: 100%|█| 275/275 [00:11<00:00, 52.47batch/s, batch_idx=219, loss=2.407, train_loss=2.37, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 22:  80%|▊| 220/275 [00:11<00:02, 19.59batch/s, batch_idx=219, loss=2.403, train_loss=2.37, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 22:  85%|▊| 233/275 [00:11<00:01, 26.18batch/s, batch_idx=219, loss=2.403, train_loss=2.37, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 22:  90%|▉| 247/275 [00:11<00:00, 34.53batch/s, batch_idx=219, loss=2.403, train_loss=2.37, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 22:  95%|▉| 261/275 [00:11<00:00, 44.58batch/s, batch_idx=219, loss=2.403, train_loss=2.37, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 22: 100%|█| 275/275 [00:11<00:00, 44.58batch/s, batch_idx=219, loss=2.403, train_loss=2.37, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 23:  80%|▊| 220/275 [00:11<00:03, 17.66batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 23:  80%|▊| 221/275 [00:11<00:02, 19.45batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 23:  84%|▊| 232/275 [00:11<00:01, 25.82batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 23:  88%|▉| 243/275 [00:11<00:00, 33.49batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 23:  93%|▉| 257/275 [00:11<00:00, 43.12batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|█| 275/275 [00:11<00:00, 55.36batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 24:  80%|▊| 220/275 [00:11<00:02, 19.34batch/s, batch_idx=219, loss=2.403, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 24:  84%|▊| 230/275 [00:11<00:01, 25.38batch/s, batch_idx=219, loss=2.403, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 24:  88%|▉| 241/275 [00:11<00:01, 32.85batch/s, batch_idx=219, loss=2.403, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 24:  91%|▉| 251/275 [00:11<00:00, 40.90batch/s, batch_idx=219, loss=2.403, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 24:  96%|▉| 263/275 [00:11<00:00, 50.68batch/s, batch_idx=219, loss=2.403, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 24: 100%|█| 275/275 [00:11<00:00, 50.68batch/s, batch_idx=219, loss=2.403, train_loss=2.42, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 25:  80%|▊| 220/275 [00:11<00:02, 19.13batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 25:  80%|▊| 221/275 [00:11<00:02, 20.47batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 25:  84%|▊| 232/275 [00:11<00:01, 27.03batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 25:  89%|▉| 245/275 [00:12<00:00, 35.31batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 25:  94%|▉| 258/275 [00:12<00:00, 45.06batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=10\u001b[A\n",
      "Epoch 25: 100%|█| 275/275 [00:12<00:00, 57.70batch/s, batch_idx=219, loss=2.405, train_loss=2.43, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 26:  80%|▊| 220/275 [00:11<00:02, 19.32batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 26:  80%|▊| 221/275 [00:11<00:02, 21.05batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 26:  85%|▊| 234/275 [00:11<00:01, 28.03batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 26:  89%|▉| 245/275 [00:11<00:00, 35.94batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 26:  94%|▉| 258/275 [00:11<00:00, 45.62batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 26: 100%|█| 275/275 [00:11<00:00, 45.62batch/s, batch_idx=219, loss=2.404, train_loss=2.41, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 27:  80%|▊| 220/275 [00:11<00:02, 20.22batch/s, batch_idx=219, loss=2.406, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 27:  80%|▊| 221/275 [00:11<00:02, 21.42batch/s, batch_idx=219, loss=2.406, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 27:  85%|▊| 233/275 [00:11<00:01, 28.39batch/s, batch_idx=219, loss=2.406, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 27:  89%|▉| 245/275 [00:11<00:00, 36.80batch/s, batch_idx=219, loss=2.406, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 27:  95%|▉| 260/275 [00:11<00:00, 47.41batch/s, batch_idx=219, loss=2.406, train_loss=2.4, v_num=44, val_acc1=8.9\u001b[A\n",
      "Epoch 27: 100%|█| 275/275 [00:11<00:00, 59.38batch/s, batch_idx=219, loss=2.406, train_loss=2.4, v_num=44, val_acc1=7.6\u001b[A\n",
      "Epoch 28:  80%|▊| 220/275 [00:11<00:02, 20.23batch/s, batch_idx=219, loss=2.405, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 28:  84%|▊| 232/275 [00:11<00:01, 26.91batch/s, batch_idx=219, loss=2.405, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 28:  89%|▉| 245/275 [00:11<00:00, 35.23batch/s, batch_idx=219, loss=2.405, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 28:  94%|▉| 258/275 [00:11<00:00, 45.06batch/s, batch_idx=219, loss=2.405, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 28: 100%|▉| 274/275 [00:11<00:00, 57.02batch/s, batch_idx=219, loss=2.405, train_loss=2.42, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 28: 100%|█| 275/275 [00:11<00:00, 57.02batch/s, batch_idx=219, loss=2.405, train_loss=2.42, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 29:  80%|▊| 220/275 [00:11<00:02, 20.22batch/s, batch_idx=219, loss=2.405, train_loss=2.36, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 29:  84%|▊| 232/275 [00:11<00:01, 26.91batch/s, batch_idx=219, loss=2.405, train_loss=2.36, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 29:  89%|▉| 245/275 [00:11<00:00, 35.20batch/s, batch_idx=219, loss=2.405, train_loss=2.36, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 29:  94%|▉| 259/275 [00:11<00:00, 45.32batch/s, batch_idx=219, loss=2.405, train_loss=2.36, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 29: 100%|█| 275/275 [00:11<00:00, 57.44batch/s, batch_idx=219, loss=2.405, train_loss=2.36, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 30:  80%|▊| 220/275 [00:11<00:02, 20.49batch/s, batch_idx=219, loss=2.405, train_loss=2.41, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 30:  82%|▊| 226/275 [00:11<00:01, 25.91batch/s, batch_idx=219, loss=2.405, train_loss=2.41, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 30:  87%|▊| 239/275 [00:11<00:01, 34.05batch/s, batch_idx=219, loss=2.405, train_loss=2.41, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 30:  92%|▉| 252/275 [00:11<00:00, 43.57batch/s, batch_idx=219, loss=2.405, train_loss=2.41, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 30:  96%|▉| 265/275 [00:11<00:00, 53.97batch/s, batch_idx=219, loss=2.405, train_loss=2.41, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 30: 100%|█| 275/275 [00:11<00:00, 53.97batch/s, batch_idx=219, loss=2.405, train_loss=2.41, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 31:  80%|▊| 220/275 [00:10<00:02, 20.47batch/s, batch_idx=219, loss=2.402, train_loss=2.38, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 31:  80%|▊| 221/275 [00:10<00:02, 21.52batch/s, batch_idx=219, loss=2.402, train_loss=2.38, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 31:  85%|▊| 234/275 [00:11<00:01, 28.70batch/s, batch_idx=219, loss=2.402, train_loss=2.38, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 31:  89%|▉| 246/275 [00:11<00:00, 36.25batch/s, batch_idx=219, loss=2.402, train_loss=2.38, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 31: 100%|█| 275/275 [00:11<00:00, 47.42batch/s, batch_idx=219, loss=2.402, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 32:  80%|▊| 220/275 [00:11<00:02, 20.17batch/s, batch_idx=219, loss=2.407, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 32:  82%|▊| 226/275 [00:11<00:01, 25.60batch/s, batch_idx=219, loss=2.407, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 32:  87%|▊| 238/275 [00:11<00:01, 33.45batch/s, batch_idx=219, loss=2.407, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 32:  91%|▉| 249/275 [00:11<00:00, 42.00batch/s, batch_idx=219, loss=2.407, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 32:  95%|▉| 261/275 [00:11<00:00, 52.06batch/s, batch_idx=219, loss=2.407, train_loss=2.44, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 32: 100%|█| 275/275 [00:11<00:00, 52.06batch/s, batch_idx=219, loss=2.407, train_loss=2.44, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 33:  80%|▊| 220/275 [00:11<00:02, 18.63batch/s, batch_idx=219, loss=2.405, train_loss=2.39, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 33:  84%|▊| 232/275 [00:11<00:01, 24.91batch/s, batch_idx=219, loss=2.405, train_loss=2.39, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 33:  89%|▉| 244/275 [00:11<00:00, 32.53batch/s, batch_idx=219, loss=2.405, train_loss=2.39, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 33:  95%|▉| 260/275 [00:11<00:00, 42.52batch/s, batch_idx=219, loss=2.405, train_loss=2.39, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 33: 100%|█| 275/275 [00:11<00:00, 42.52batch/s, batch_idx=219, loss=2.405, train_loss=2.39, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 34:  80%|▊| 220/275 [00:11<00:02, 19.91batch/s, batch_idx=219, loss=2.404, train_loss=2.42, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 34:  82%|▊| 225/275 [00:11<00:02, 24.66batch/s, batch_idx=219, loss=2.404, train_loss=2.42, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 34:  86%|▊| 236/275 [00:11<00:01, 32.10batch/s, batch_idx=219, loss=2.404, train_loss=2.42, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 34:  91%|▉| 249/275 [00:11<00:00, 41.28batch/s, batch_idx=219, loss=2.404, train_loss=2.42, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 34:  96%|▉| 265/275 [00:11<00:00, 52.90batch/s, batch_idx=219, loss=2.404, train_loss=2.42, v_num=44, val_acc1=8.\u001b[A\n",
      "Epoch 34: 100%|█| 275/275 [00:11<00:00, 52.90batch/s, batch_idx=219, loss=2.404, train_loss=2.42, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 35:  80%|▊| 220/275 [00:11<00:02, 19.92batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 35:  82%|▊| 226/275 [00:11<00:01, 25.35batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:  87%|▊| 239/275 [00:11<00:01, 33.40batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 35:  92%|▉| 252/275 [00:11<00:00, 42.95batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 35: 100%|█| 275/275 [00:12<00:00, 54.37batch/s, batch_idx=219, loss=2.403, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 36:  80%|▊| 220/275 [00:11<00:02, 20.30batch/s, batch_idx=219, loss=2.406, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 36:  82%|▊| 226/275 [00:11<00:01, 25.74batch/s, batch_idx=219, loss=2.406, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 36:  87%|▊| 239/275 [00:11<00:01, 33.88batch/s, batch_idx=219, loss=2.406, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 36:  92%|▉| 252/275 [00:11<00:00, 43.33batch/s, batch_idx=219, loss=2.406, train_loss=2.38, v_num=44, val_acc1=9.\u001b[A\n",
      "Epoch 36: 100%|█| 275/275 [00:11<00:00, 55.03batch/s, batch_idx=219, loss=2.406, train_loss=2.38, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 37:  80%|▊| 220/275 [00:11<00:02, 19.24batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 37:  84%|▊| 230/275 [00:11<00:01, 25.34batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 37:  88%|▉| 241/275 [00:11<00:01, 32.72batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 37:  93%|▉| 256/275 [00:11<00:00, 42.57batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 37:  98%|▉| 269/275 [00:11<00:00, 53.22batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=7.\u001b[A\n",
      "Epoch 37: 100%|█| 275/275 [00:11<00:00, 53.22batch/s, batch_idx=219, loss=2.404, train_loss=2.39, v_num=44, val_acc1=6.\u001b[A\n",
      "Epoch 38:  51%|▌| 140/275 [00:07<00:07, 18.68batch/s, batch_idx=139, loss=2.405, train_loss=2.39, v_num=44, val_acc1=6.                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-871155ec8f98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 3 START TRAINING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# ------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;31m# return 1 when finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;31m# CORE TRAINING LOOP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 829\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# RUN TNG EPOCH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[1;31m# -----------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# update LR schedulers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# RUN TRAIN STEP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# ---------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# update progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_progress_bar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_progress_bar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_tqdm_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# collapse all metrics into one dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[1;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[0;32m   1388\u001b[0m                                  for key in postfix.keys())\n\u001b[0;32m   1389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mprint_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mfp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[0mfp_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;31m# request flush on the background thread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             \u001b[1;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[1;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\myphd_env\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = ptl.Trainer(early_stop_callback=False)\t\n",
    "model = CoolModel()\n",
    "# ------------------------\n",
    "# 3 START TRAINING\n",
    "# ------------------------\n",
    "trainer.fit(model)\n",
    "trainer.run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17900), started 0:30:03 ago. (Use '!kill 17900' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-134d22b63b3f1bfa\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-134d22b63b3f1bfa\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
